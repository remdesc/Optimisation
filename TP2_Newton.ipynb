{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"6\"> The gradient descent algorithm </font> (second part)</h1>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li><a href=\"#prelim\">Preliminaries</a></li>\n",
    "        <li><a href=\"#Newton\">The Newton method</a></li>\n",
    "        <li><a href=\"#BFGS\">A Quasi-Newton Meton (BFGS)</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelim'></a>\n",
    "<h2>Preliminaries</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required libraries. We define two functions used to visualize *1/* the level lines of the objective function and *2/* their gradient vector fields (when they depend on 2 variables). There is also two examples of plots used to observe the convergence of the optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def draw_vector_field(F, xmin, xmax, ymin, ymax, N=15):\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\n",
    "    M[M == 0] = 1  # avoid division by 0\n",
    "    U /= M  # normalize the u componant\n",
    "    V /= M  # normalize the v componant\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\n",
    "\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "    z = f(*np.meshgrid(x, y))\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \n",
    "\n",
    "f = lambda x, y : np.cosh(x)+ np.sin(x + y)**2\n",
    "df = lambda x, y : np.array([np.sinh(x) + 2*np.cos(x + y)*np.sin(x + y), 2*np.cos(x + y)*np.sin(x + y)])\n",
    "%matplotlib inline\n",
    "level_lines(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, -1.1, 1.1, -1.1, 1.1, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "N=100\n",
    "t = np.linspace(0,2*np.pi,N, endpoint='False')\n",
    "x, y, z = np.cos(t), np.sin(t), np.sin(3*t+np.pi/4)\n",
    "ax = Axes3D(plt.figure())  # Define the 3D plot\n",
    "ax.set(xlabel=r'$x$', ylabel=r'$y$', zlabel=r'$z$')\n",
    "ax.plot(x, y, z,'.')  # Plot of the trajectory\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "N = 10\n",
    "F = 2**(-np.linspace(0,N,N+1))\n",
    "plt.figure()\n",
    "plt.semilogy(range(N + 1), F, '.', linestyle='dashed')\n",
    "\n",
    "len(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Newton'></a>\n",
    "<h2>The Newton method</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton (or Newton-Raphson) method is an iterative descent method where the descent direction at step $k$ is chosen in order to minimize the second order Taylor expansion of $f$ around the current point $x^k$, namely,\n",
    "$$\n",
    "\\tag{4}\n",
    "m_k(d):=f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T D^2 f(x^k) d.\n",
    "$$\n",
    "If the matrix $D^2 f(x^k)$ is invertible (in particular if it is positive definite) the minimizer of $m^k$ exists and is unique. We note $H^k$ the inverse of $D^2 f(x^k)$, $g^k:=\\nabla f(x^k)$ and $d^k$ the minimizer of (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 13.*** Express $d^k$ as a function of $H^k$ and $g^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Solution:___ \n",
    "\n",
    "$d^k$ minimizes $m_k(d)$ so ‚àá$m_k(d^k)$ = 0\n",
    "\n",
    "$d^k$ verifies the equation : ‚àáùëì($x^k$) + ùê∑¬≤ùëì($x^k$) $d^k$ = 0\n",
    "\n",
    "so $d^k$ = - $[D^2 f(x^k)]^{-1}$ ‚àáùëì($x^k$)  = - $H^k$ $g^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the descent direction $d^k$ has been computed, we proceed as in the gradient descent method and use a line-search method to find $t$ such that $f(x^k+ t d^k)$ is sufficienlty smaller than $f(x^k)$. We will use again the Armijo criterion:  \n",
    "\n",
    "$$\n",
    "\\tag{5} \n",
    "f(x^k+t d^k)\\, \\le\\, f(x^k) + \\alpha\\, t \\left<d^k;g^k\\right>.\n",
    "$$\n",
    "\n",
    "Then the Newton-Raphson algorithm with Armijo criterion runs as follows: Fix $\\alpha \\in (0,1)$ and pick some $x^0\\in {\\mathbb R}^N$. Then for $k=0,1,\\ldots$, up to convergence, repeat: \n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{l}\n",
    "\\text{Compute }d^k,\\\\\n",
    "\\text{Find }\\tau^k>0 \\text{ such that (5) holds true for }t=\\tau^k,\\\\\n",
    "\\text{Set }x^{k+1}:= x^k + \\tau^k d^k.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "An important point here is that, when $x^k$ is sufficiently close to a local minimizer $x^*$ of $f$ which is such that $D^2f(x^*)$ is positive definite, then the choice $\\tau^k=1$ provides a quadratic convergence to $x^*$, _i.e._ \n",
    "$$\n",
    "\\|x^{k+1}-x^*\\|\\leq C \\|x^k - x^*\\|^2.\n",
    "$$\n",
    "If we don't want to loose this super-linear behavior, we need to pick $\\tau^k=1$. To achieve this, we start the back-tracking iterations with $\\tau^k_0=1$ and we choose a small parameter in (5) ($\\alpha=0.1$ for instance).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Lambda>0$. We define $f_\\Lambda(x,y):=(1-x)^2 + \\Lambda\\,(y-x^2)^2$, for $(x,y)\\in\\mathbb{R}^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 14.__ Compute $\\nabla f_\\Lambda(x,y)$. Find the minimizer(s) of $f_\\Lambda$. Plot some level lines of $f_\\Lambda$ together with the renormalized vector field $(1/|\\nabla f_\\Lambda|)\\nabla f_\\Lambda$ for $\\Lambda=100$. Compute $D^2 f(x,y)$ and its inverse $H_\\Lambda(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 100\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x, y: np.array([2*(x-1)-Lambda*4*(y-x**2)*x,2*Lambda*(y-x**2)])\n",
    "ddf = lambda x,y : np.array([[2 - 4*Lambda*(y-3*x**2)  , -4*Lambda*x], [-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y : np.linalg.inv(np.array([[2 - 4*Lambda*(y-3*x**2)  , -4*Lambda*x], [-4*Lambda*x, 2*Lambda]]))\n",
    "\n",
    "level_lines(f, .8, 1.2, 0.8, 1.2, np.linspace(0, 30, 80))\n",
    "draw_vector_field(df, .8, 1.2, 0.8, 1.2, 15)\n",
    "plt.plot(1,1,'or')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimizer of the function ùëìŒõ is (1, 1).\n",
    "\n",
    "Indeed, ‚àáùëìŒõ(ùë•,ùë¶) = 0 \n",
    "\n",
    "‚áî 2(x-1) = 4Œõx(y-x¬≤) and 2Œõ(y-x¬≤) = 0 \n",
    "\n",
    "‚áî x¬≤ = y and 2(x-1) = 0 \n",
    "\n",
    "‚áî x = 1 and y = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 15.__ Implement the Newton method and apply it to the above function with $\\alpha = 0.1$, $\\beta=0.75$ and $x^0=(0,0)$. Represent the iterations on a graph and plot $\\ \\log(f_\\Lambda(x^k))\\ $ as a function of $k$. Observe and comment.\n",
    "\n",
    "_Hint:_ First test the algorithm on the quadratic function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the test\n",
    "f = lambda x,y : ( x - 1)**2 + 2*(y - 1)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) , 4*(y - 1)])\n",
    "ddf = lambda x,y : np.array([[2  , 0], [0, 2]])\n",
    "HH = lambda x,y : np.array([[.5, 0], [0, .25]])\n",
    "\n",
    "## Parameters\n",
    "alpha, beta = .1, .75\n",
    "epsilon = 1e-8\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "## initialization \n",
    "iter = 0\n",
    "x = (0, 0)\n",
    "t=1\n",
    "\n",
    "Z, F =[[x[0],x[1]]], [f(x[0], x[1])]\n",
    "flag = 'OK'\n",
    "\n",
    "## Optmization loop\n",
    "while (iter < itermax):\n",
    "    fz = f(x[0],x[1])\n",
    "    d = -HH(x[0],x[1])@df(x[0],x[1])\n",
    "    g = df(x[0],x[1])\n",
    "    if np.hypot(g[0], g[1]) < epsilon or flag == 'Not OK':\n",
    "        break\n",
    "    new_fz = f(x[0] + t*d[0], x[1] + t*d[1]) \n",
    "    iter_ls = 0\n",
    "    while (new_fz - fz - alpha*t*(d.T)@g >=0):\n",
    "        t=beta*t\n",
    "        new_fz = f(x[0] + t*d[0], x[1] + t*d[1])\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    #print(\"d = \" + str(d) + \", f(z) - 1 =\" + str(fz-1) + \", t= \" +str(t))\n",
    "    x, fz = (x[0] + t*d[0], x[1] + t*d[1]), new_fz\n",
    "    Z.append([x[0], x[1]])\n",
    "    F.append(fz)\n",
    "    t = 1\n",
    "    iter += 1\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range (len(Z)):\n",
    "    X.append(Z[i][0])\n",
    "    Y.append(Z[i][1])\n",
    "F = np.array(F)\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(X, Y,'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "#plt.figure(figsize=(9,6))\n",
    "#plt.semilogy(range(len(F)),F,'.',linestyle='dashed')\n",
    "#plt.show()\n",
    "\n",
    "print(\"nbre d'it√©rations :\", iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the quadratic function, the Newton method converges directly (1 iteration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the function fŒõ\n",
    "Lambda = 100\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x, y: np.array([2*(x-1)-Lambda*4*(y-x**2)*x,2*Lambda*(y-x**2)])\n",
    "ddf = lambda x,y : np.array([[2 - 4*Lambda*(y-3*x**2)  , -4*Lambda*x], [-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y : np.linalg.inv(np.array([[2 - 4*Lambda*(y-3*x**2)  , -4*Lambda*x], [-4*Lambda*x, 2*Lambda]]))\n",
    "\n",
    "## Parameters\n",
    "alpha, beta = .1, .75\n",
    "epsilon = 1e-8\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "## initialization \n",
    "iter = 0\n",
    "x = (0, 0)\n",
    "t=1\n",
    "\n",
    "Z, F =[[x[0],x[1]]], [f(x[0], x[1])]\n",
    "flag = 'OK'\n",
    "\n",
    "## Optmization loop\n",
    "while (iter < itermax):\n",
    "    fz = f(x[0],x[1])\n",
    "    d = -HH(x[0],x[1])@df(x[0],x[1])\n",
    "    g = df(x[0],x[1])\n",
    "    if np.hypot(g[0], g[1]) < epsilon or flag == 'Not OK':\n",
    "        break\n",
    "    new_fz = f(x[0] + t*d[0], x[1] + t*d[1]) \n",
    "    iter_ls = 0\n",
    "    while (new_fz - fz - alpha*t*(d.T)@g >=0):\n",
    "        t=beta*t\n",
    "        new_fz = f(x[0] + t*d[0], x[1] + t*d[1])\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    #print(\"d = \" + str(d) + \", f(z) - 1 =\" + str(fz-1) + \", t= \" +str(t))\n",
    "    x, fz = (x[0] + t*d[0], x[1] + t*d[1]), new_fz\n",
    "    Z.append([x[0], x[1]])\n",
    "    F.append(fz)\n",
    "    t = 1\n",
    "    iter += 1\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range (len(Z)):\n",
    "    X.append(Z[i][0])\n",
    "    Y.append(Z[i][1])\n",
    "F = np.array(F)\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(X, Y,'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(range(1,len(F)),F[1:len(F)],'.',linestyle='dashed')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"log(f(xk))\")\n",
    "plt.show()\n",
    "\n",
    "print(\"nbre d'it√©rations :\", iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton method needs 13 iterations to converge to the minimizer of the function ùëìŒõ with a precision of epsilon = 10^-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BFGS'></a>\n",
    "<h2> A Quasi-Newton Meton (BFGS)</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of parameters is large as it is usual in Machine Learning, computing the Hessian matrices $D^2f(x^k)$  and solving the linear systems $D^2f(x^k) d^k=-g^k$ may be too costly. However it is often still possible to achieve superlinear convergence by replacing $[D^2f(x^k)]^{-1}$ by a cheap approximation $H^k$.  There exist several algorithms based on this idea. We present one of the most popular: the BFGS method named after their discoverers (Broyden, Fletcher, Goldfarb and Shanno). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explain the method. Assume that at step $k$ we have symmetric positive definite approximation $H^k$ of $\\left[D^2f(x^k)\\right]^{-1}$. We note $B^k$ its inverse (which is an approximation of $D^2f(x^k)$). As above, we define our descent direction $d^k$ as the minimizer of \n",
    "$$\n",
    "f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T B^k d.\n",
    "$$\n",
    "This leads to\n",
    "$$\n",
    "d^k = -\\left[B^k\\right]^{-1} \\nabla f(x^k) = - H^k g^k. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find $\\tau^k$ satisfying (5) and we set \n",
    "$$\n",
    "x^{k+1} := x^k +\\tau^k d^k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need an approximation $H^{k+1}$ of $\\left[D^2f(x^{k+1})\\right]^{-1}$. For this, let us recall that we want \n",
    "$$\n",
    "\\tilde m_{k+1} (d):= f(x^{k+1}) + g^{k+1}\\cdot d +\\dfrac 12 d^T B^{k+1} d,\n",
    "$$\n",
    "to be an approximation of \n",
    "$$\n",
    "\\overline m_{k+1}(d):= f(x^{k+1} + d).\n",
    "$$\n",
    "We already have by construction, \n",
    "$$\n",
    "\\tilde m_{k+1}(0)=\\overline m_{k+1}(0)=f(x^{k+1})\\qquad\\text{and}\\qquad \\nabla \\tilde m_{k+1}(0)=\\nabla \\overline m_{k+1}(0)=g(x^{k+1}).\n",
    "$$\n",
    "We enforce the new condition\n",
    "$$\n",
    "\\nabla m_{k+1}(-\\tau_k d^k)=\\nabla \\overline m_{k+1}(-\\tau_k d^k)=g^k.\n",
    "$$\n",
    "\n",
    "Noting $a^k:=g^{k+1}-g^k$ and $b^k:=\\tau^kd^k=x^{k+1}-x^k$, this is equivalent to $B^{k+1}b^k=a^k$. Assuming $B^{k+1}$ is invertible, this is equivalent to: $H^{k+1}$ solves\n",
    "\n",
    "$$\n",
    "\\tag{6}\n",
    "Ha^k=b^k.\n",
    "$$\n",
    "\n",
    "A necessary and sufficient condition for  (6) to admit a symmetric positive definite solution $H$ is the condition \n",
    "\n",
    "$$\n",
    "\\tag{7}\n",
    "\\left<a^k;b^k\\right> >0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to destroy all the information contained in $H^k$, so, assuming that (7) holds true, we choose a solution of (6) as close as possible of $H^k$. A popular choice is to set:\n",
    "$$\n",
    "\\tag{8}\n",
    "H^{k+1} := \\left(I-\\rho_k b^k\\otimes a^k\\right) H^k \\left(I-\\rho_k a^k\\otimes b^k\\right) + \\rho_k b^k\\otimes b^k,\\quad\\text{ with }\\quad \\rho_k:=\\dfrac1{\\left<a^k;b^k\\right>}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 16.__ Check that formula (8) provides a solution to (6). Check that $H^{k+1}$ defined this way is a symmetric positive definite matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Solution:___\n",
    "Assuming that $H^k$ is positive definite and that (7) is true, it is easy to check that $H^{k+1}$ is also positive definite. First, for $d\\in \\mathbb{R}^N$, noting $w=d-\\rho_k \\left<b^k;d\\right>a^k$, we compute,\n",
    "\n",
    "$$\n",
    " d^TH^{k+1}d =  w^TH^kw + \\rho_k  \\left<d;b^k\\right>^2\\ \\geq \\ 0.\n",
    "$$\n",
    "\n",
    "Next, using the same formula, we see that $d^TH^{k+1}d=0$ implies $w=0$ and $\\left<d;b^k\\right>=0$. The former implies that $d=\\lambda a^k$ for some $\\lambda\\in \\mathbb{R}$ and with the latter this yields $\\lambda\\left<a^k;b^k\\right>=0$. Hence $\\lambda=0$ and $d=0$. This proves that $H^{k+1}$ is positive definite.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 17.__ Implement the BFGS method and apply it to the above function with $\\alpha = 0.1$, $\\beta=0.75$ and $x^0=(0,0)$. Start with $H^0=I$.\n",
    "\n",
    "Represent the iterations on a graph and plot $\\ \\log(f(x^k))\\ $ as a function of $k$. Observe and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bfgs1(H, x_old, x, g_x_old, g_x):       #g_x:gradient en x_(k+1)  g_x_old : gradient de f en x_k\n",
    "    a = g_x - g_x_old\n",
    "    b = x - x_old\n",
    "    phi = 1/(a.T@b)\n",
    "    I = np.eye(2)\n",
    "    return (I-phi*(np.outer(b,a)))@H@(I-phi*(np.outer(a,b)))+phi*(np.outer(b,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "alpha, beta = .1, .75\n",
    "epsilon = 1e-8\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "##\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=\"True\")\n",
    "\n",
    "## initialization \n",
    "iter = 0\n",
    "x = np.array([0, 0])\n",
    "t=1\n",
    "\n",
    "Z, F =[[x[0],x[1]]], [fz]\n",
    "flag = 'OK'\n",
    "H=np.eye(2)\n",
    "liste_H = []\n",
    "liste_H.append(H)\n",
    "\n",
    "## Optmization loop       \n",
    "while (iter < itermax):\n",
    "    fz = f(x[0],x[1])\n",
    "    d = -H@df(x[0],x[1])\n",
    "    g = df(x[0],x[1])\n",
    "    if np.hypot(g[0], g[1]) < epsilon or flag == 'Not OK':\n",
    "        break\n",
    "    new_fz = f(x[0] + t*d[0], x[1] + t*d[1]) \n",
    "    iter_ls = 0\n",
    "    while (new_fz - fz - alpha*t*(d.T)@g >=0):\n",
    "        t=beta*t\n",
    "        new_fz = f(x[0] + t*d[0], x[1] + t*d[1])\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    #print(\"d = \" + str(d) + \", f(z) - 1 =\" + str(fz-1) + \", t= \" +str(t))\n",
    "    H=update_bfgs1(H, x, np.array([x[0] + t*d[0], x[1] + t*d[1]]), df(x[0],x[1]), df(x[0] + t*d[0],x[1] + t*d[1]))\n",
    "    liste_H.append(H)\n",
    "    x, fz = [x[0] + t*d[0], x[1] + t*d[1]], new_fz\n",
    "    Z.append([x[0], x[1]])\n",
    "    F.append(fz)\n",
    "    t = 1\n",
    "    iter += 1\n",
    "    \n",
    "inv_hessian = HH(x[0],x[1])\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range (len(Z)):\n",
    "    X.append(Z[i][0])\n",
    "    Y.append(Z[i][1])\n",
    "F = np.array(F)\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(X, Y,'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(range(1,len(F)),F[1:len(F)],'.',linestyle='dashed')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"log(f(xk))\")\n",
    "plt.show()\n",
    "\n",
    "print(\"nbre d'it√©rations :\", iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence of the BFGS method is slower than the Newton method. Indeed, now we need 22 iterations to converge to the minimizer with the same precision, whereas the Newton method needs 13 iterations. The convergence is slower because we approximated the inverse of the Hessian matrix by H, instead of taking its real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 18.__ Does $H^k$ converge to  $[D^2 f(x^*)]^{-1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Frobenius norm to check the convergence of $H^k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norme_matrice (A):\n",
    "    return np.sqrt(np.trace(A.T@A))\n",
    "\n",
    "\n",
    "norme = []\n",
    "for i in range (len(liste_H)):\n",
    "    norme.append(norme_matrice(liste_H[i]-inv_hessian))\n",
    "\n",
    "# plot of the values of ||Happrox-Htrue|| along the iterations.\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(range(len(norme)),norme,'.',linestyle='dashed')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"||H_k -(D¬≤f(x*))^-1||\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(range(len(norme)),norme,'.',linestyle='dashed')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"log(||H_k -(D¬≤f(x*))^-1||)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the difference in norm between $H^k$ and  $[D^2 f(x^*)]^{-1}$ becomes very low when k grows. As a consequence, we can assert that $H^k$ converges to  $[D^2 f(x^*)]^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
